# -*-Python-*-
import ddsp
import ddsp.training

# Globals for easier configuration with --gin_param
learning_rate = 3e-4
batch_size = 32
lr_decay_steps = 10000
lr_decay_rate = 0.98
grad_clip_norm = 3.0
checkpoints_to_keep = 100

train.batch_size = %batch_size
train.num_steps = 1000000
train.steps_per_summary = 300
train.steps_per_save = 300

Trainer.learning_rate = %learning_rate
Trainer.lr_decay_steps = %lr_decay_steps
Trainer.lr_decay_rate = %lr_decay_rate
Trainer.grad_clip_norm = %grad_clip_norm
Trainer.checkpoints_to_keep = %checkpoints_to_keep
